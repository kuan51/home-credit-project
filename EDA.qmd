---
title: "Home Credit Default: EDA"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false
---

# Setup

```{r}
# Load dependencies
library(knitr)
library(tidyverse)
library(patchwork)

# Print class info
class_info <- tibble(student = "Rex Linder", course = "6910-090")
kable(class_info)

# Load Data
df <- read_csv("./data/application_train.csv")
test <- read_csv("./data/application_test.csv")
```

# Introduction

## Dataset Description

This analysis uses the [Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk) dataset from Kaggle. Home Credit is an international consumer finance provider operating in Europe and Asia, focused on lending to consumers with little or no credit history.

The dataset consists of multiple related tables:

```{r}
# Define data files and descriptions
data_files <- tribble(
  ~file,                      ~description,
  "application_train.csv",    "Main application data with target",
  "application_test.csv",     "Test set for predictions",
  "bureau.csv",               "Credit bureau history",
  "bureau_balance.csv",       "Monthly bureau balance",
  "credit_card_balance.csv",  "Credit card snapshots",
  "installments_payments.csv","Payment history",
  "POS_CASH_balance.csv",     "POS/cash loan snapshots",
  "previous_application.csv", "Previous HC applications"
)

# Function to get file dimensions efficiently
get_file_info <- function(filename) {
  path <- file.path("data", filename)
  # Read just the header to get column count
  header <- read_csv(path, n_max = 0, show_col_types = FALSE)
  n_cols <- ncol(header)
  # Count rows efficiently
  n_rows <- length(count.fields(path, sep = ",")) - 1
  tibble(rows = n_rows, cols = n_cols)
}

# Build the summary table
dataset_summary <- data_files |>
  mutate(info = map(file, get_file_info)) |>
  unnest(info) |>
  mutate(
    Table = str_remove(file, "\\.csv$"),
    Records = case_when(
      rows >= 1e6 ~ sprintf("%.1fM", rows / 1e6),
      TRUE ~ format(rows, big.mark = ",")
    ),
    Features = cols,
    Description = description
  ) |>
  select(Table, Records, Features, Description)
# Display summary
kable(dataset_summary, align = c("l", "r", "r", "l"))
```

## Objective of Analysis

The business objectives are:

1. Reduce loan defaults by improving credit decision accuracy
2. Increase approvals for creditworthy applicants (minimize false rejections)
3. Reduce cost of capital through better risk provisioning
4. Ensure fairness by avoiding discrimination against protected classes

Success is measured by decreasing the default rate by at least 10% while maintaining or increasing approval volume for creditworthy customers.

## Target Variable

There are ~91.9% instances of 0 in the target variable. As defaulting is not the norm, we can infer that 0 means the borrower paid back the loan and 1 indicates a default.

```{r}
# Get percentage of repaid/default
percent_defaulted <- df |> 
  count(TARGET) |>
  mutate(
    pct = (n / sum(n)) * 100,
    label = if_else(TARGET == 0, "Repaid", "Default")
  )
kable(percent_defaulted, digits=1)
```

# Data Preprocessing

Before analysis, we examine the dataset's structure, data types, and organization to understand what we're working with and identify potential issues that may need addressing.

## Inspect Data Structure and Types

The following code outputs help quickly understand the data types and sample the corresponding values.

```{r}
# Get number of columns and rows
cat("Number rows: ", nrow(df), "\nNumber columns: ", ncol(df))
# Review Data Types
numeric_cols <- df |> select(where(is.numeric)) |> names()
character_cols <- df |> select(where(is.character)) |> names()
cat("Numeric columns:", length(numeric_cols), "\nCharacter columns:", length(character_cols))
glimpse(df)
```

## Column Categories

The 122 columns can be grouped into logical categories based on their prefixes.

```{r}
# Group columns into logical categories
column_categories <- tibble(column = names(df)) |>
  mutate(
    prefix = str_extract(column, "^[A-Z]+(?=_)"),                                                                          
    category = case_when(        
      prefix == "FLAG" ~ "Flags",
      prefix == "AMT" ~ "Amounts",
      prefix %in% c("DAYS", "YEARS") ~ "Time",
      prefix %in% c("NAME", "CODE", "CNT", "OCCUPATION") ~ "Demographics",
      prefix %in% c("APARTMENTS", "LIVINGAREA", "BASEMENTAREA", "FLOORSMAX",
                    "FLOORSMIN", "ELEVATORS", "ENTRANCES", "COMMONAREA",
                    "LANDAREA", "NONLIVINGAREA", "LIVINGAPARTMENTS",
                    "NONLIVINGAPARTMENTS", "TOTALAREA", "WALLSMATERIAL",
                    "FONDKAPREMONT", "HOUSETYPE", "EMERGENCYSTATE") ~ "Housing",
      prefix %in% c("EXT", "REG", "OBS", "DEF") ~ "External Sources",
      prefix == "SK" ~ "Identifiers",
      prefix == "REGION" ~ "Region",
      prefix %in% c("LIVE", "WEEKDAY", "HOUR", "OWN", "ORGANIZATION") ~ "Other",
      is.na(prefix) ~ "Other"
    )
  )

# Visual summary of categories
column_categories |>
  count(category) |>
  ggplot(aes(x = reorder(category, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(x = NULL, y = "Number of Columns", title = "Columns by Category")
```

# Data Overview

This section examines the training data to understand the central tendencies, ranges, and potential anomalies in key numeric variables.

## Summary Statistics

There are outliers and anomalies in the data. For example, EMPLOYED_YEARS has negative values with the minimum being ~1,000 years! This must be some sort of placeholder for clients who are unemployed or reliant on pensions. Client income also shows some outliers. Few clients earn 117 million per year and this column is skewed by those high earners.

```{r}
df |>
  # Select key columns and generate summary table
  select(AMT_INCOME_TOTAL, AMT_CREDIT, AMT_ANNUITY, AMT_GOODS_PRICE, DAYS_BIRTH, DAYS_EMPLOYED) |>
  mutate(
    # Convert days to years old
    AGE_YEARS = -DAYS_BIRTH / 365,
    EMPLOYED_YEARS = -DAYS_EMPLOYED / 365
  ) |>
  # Drop day columns and only show human readable years
  select(-DAYS_BIRTH, -DAYS_EMPLOYED) |>
  summary()
```

# Exploratory Data Analysis

This section investigates data quality issues, variable distributions, and relationships between features and the target variable to inform modeling decisions.

## Missing Values

Some columns are missing a lot of data! Some of these columns should be ignored as they are not neccesarily useful. Other columns require imputation of plausible values when a NA does occur in the dataset.

```{r}
#| fig-height: 14

# Calculate NA summary
missing_summary <- df |>
  summarise(across(everything(), ~sum(is.na(.)))) |>
  pivot_longer(everything(), names_to = "column", values_to = "missing") |>
  mutate(pct_missing = missing / nrow(df) * 100) |>
  filter(missing > 0) |>
  arrange(desc(pct_missing))

# Visualize all columns with missing data
missing_summary |>
  ggplot(aes(x = reorder(column, pct_missing), y = pct_missing)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(x = NULL, y = "% Missing", title = "Columns with Missing Data") +
  theme(axis.text.y = element_text(size = 7))
```

## Duplicate Records

There are no duplicated columns or rows in the training data.

```{r}
# Check for duplicate IDs
cat("Unique IDs:", n_distinct(df$SK_ID_CURR), 
  "\nTotal rows:", nrow(df), 
  "\nIDs are unique:", n_distinct(df$SK_ID_CURR) == nrow(df))
# Check for fully duplicate rows
cat("Duplicate rows:", sum(duplicated(df)))
```

## Invalid or Inconsistent Values

Days employed shows a lot of clients as employed for 1000 years. This is a placeholder likely for unemployment and skews the column. The count of children also skews heavily due to the fact most people in this dataset have 0 children. XNA in the gender codes likely indicates a missing value.

```{r}
# All instances of DAYS_BIRTH should be less than 0, indicating the days before applying for a loan.
count_days_birth <- df |>
  count(DAYS_BIRTH >= 0)
kable(count_days_birth)

# Get count of employees with days employed equal to 365243
count_days_employed <- df |>
  count(DAYS_EMPLOYED == 365243)
kable(count_days_employed)

# CNT_CHILDREN range
df |>
    count(CNT_CHILDREN) |>
    ggplot(aes(x = factor(CNT_CHILDREN), y = n)) +
    geom_col(fill = "steelblue") +
    labs(title = "Children Count Distribution", x = "Number of Children", y = "Count")

# Income should not be less than 0
count_amt_total_income <- df |>
  count(AMT_INCOME_TOTAL <= 0)
kable(count_amt_total_income)

# Count of gender codes
count_code_gender <- df |>
  count(CODE_GENDER)
kable(count_code_gender)
```

## Univariate Analysis

Univariate analysis examines each variable individually to understand its distribution, central tendency, and spread. This helps identify skewness, outliers, and the overall shape of the data before exploring relationships between variables.

### Numeric Variables - Amount Distributions

```{r}
#| fig-height: 6
# Faceted histogram of amount variables
df |>
  select(AMT_INCOME_TOTAL, AMT_CREDIT, AMT_ANNUITY, AMT_GOODS_PRICE) |>
  pivot_longer(everything(), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white") +
  facet_wrap(~variable, scales = "free") +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of Amount Variables", x = NULL, y = "Count")
```

### Income Distribution (Log Scale)

Income is highly skewed, so a log scale reveals the distribution better:

```{r}
ggplot(df, aes(x = AMT_INCOME_TOTAL)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white") +
  scale_x_log10(labels = scales::comma) +
  labs(title = "Income Distribution (Log Scale)",
       x = "Total Income", y = "Count")
```

### Age Distribution

```{r}
df |>
  mutate(AGE_YEARS = -DAYS_BIRTH / 365) |>
  ggplot(aes(x = AGE_YEARS)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  labs(title = "Age Distribution of Applicants",
       x = "Age (Years)", y = "Count")
```

### Categorical Variables

```{r}
#| fig-height: 8
# Income type distribution
p1 <- df |>
  count(NAME_INCOME_TYPE) |>
  ggplot(aes(x = reorder(NAME_INCOME_TYPE, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Applicants by Income Type", x = NULL, y = "Count")

# Education level distribution
p2 <- df |>
  count(NAME_EDUCATION_TYPE) |>
  ggplot(aes(x = reorder(NAME_EDUCATION_TYPE, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Applicants by Education Level", x = NULL, y = "Count")

# Gender distribution
p3 <- df |>
  count(CODE_GENDER) |>
  ggplot(aes(x = CODE_GENDER, y = n)) +
  geom_col(fill = "steelblue") +
  labs(title = "Applicants by Gender", x = NULL, y = "Count")

# Family status distribution
p4 <- df |>
  count(NAME_FAMILY_STATUS) |>
  ggplot(aes(x = reorder(NAME_FAMILY_STATUS, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Applicants by Family Status", x = NULL, y = "Count")

(p1 + p2) / (p3 + p4)
```

## Bivariate Analysis

Bivariate analysis examines relationships between two variables, specifically how features relate to the target variable. This identifies which variables have predictive power and may be useful in building the model.

### Default Rate by Categorical Variables

```{r}
#| fig-height: 8

# Default rate by income type
p1 <- df |>
  group_by(NAME_INCOME_TYPE) |>
  summarise(default_rate = mean(TARGET) * 100, n = n()) |>
  ggplot(aes(x = reorder(NAME_INCOME_TYPE, default_rate), y = default_rate)) +
  geom_col(fill = "tomato") +
  coord_flip() +
  labs(title = "Default Rate by Income Type", x = NULL, y = "Default Rate (%)")

# Default rate by education
p2 <- df |>
  group_by(NAME_EDUCATION_TYPE) |>
  summarise(default_rate = mean(TARGET) * 100, n = n()) |>
  ggplot(aes(x = reorder(NAME_EDUCATION_TYPE, default_rate), y = default_rate)) +
  geom_col(fill = "tomato") +
  coord_flip() +
  labs(title = "Default Rate by Education", x = NULL, y = "Default Rate (%)")

# Default rate by gender
p3 <- df |>
  group_by(CODE_GENDER) |>
  summarise(default_rate = mean(TARGET) * 100, n = n()) |>
  ggplot(aes(x = CODE_GENDER, y = default_rate)) +
  geom_col(fill = "tomato") +
  labs(title = "Default Rate by Gender", x = NULL, y = "Default Rate (%)")

# Default rate by car ownership
p4 <- df |>
  group_by(FLAG_OWN_CAR) |>
  summarise(default_rate = mean(TARGET) * 100, n = n()) |>
  ggplot(aes(x = FLAG_OWN_CAR, y = default_rate)) +
  geom_col(fill = "tomato") +
  labs(title = "Default Rate by Car Ownership", x = NULL, y = "Default Rate (%)")

(p1 + p2) / (p3 + p4)
```

### Numeric Variables by Target

```{r}
#| fig-height: 6
df |>
  mutate(
    TARGET = factor(TARGET, labels = c("Repaid", "Default")),
    AGE_YEARS = -DAYS_BIRTH / 365
  ) |>
  select(TARGET, AMT_INCOME_TOTAL, AMT_CREDIT, AGE_YEARS) |>
  pivot_longer(-TARGET, names_to = "variable", values_to = "value") |>
  ggplot(aes(x = TARGET, y = value, fill = TARGET)) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_y") +
  scale_fill_manual(values = c("Repaid" = "steelblue", "Default" = "tomato")) +
  labs(title = "Key Variables by Loan Outcome", x = NULL, y = NULL) +
  theme(legend.position = "none")
```

## Multivariate Analysis

### Correlation Matrix

A correlation matrix reveals linear relationships between numeric variables, with values ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation). This analysis helps identify which features move together, potential multicollinearity issues for modeling, and variables that may be predictive of the target.

```{r}
#| fig-height: 8
#| fig-width: 10

# Select numeric columns for correlation
numeric_for_corr <- df |>
  select(TARGET, AMT_INCOME_TOTAL, AMT_CREDIT, AMT_ANNUITY, AMT_GOODS_PRICE,
         DAYS_BIRTH, DAYS_EMPLOYED, DAYS_REGISTRATION, DAYS_ID_PUBLISH,
         CNT_CHILDREN, CNT_FAM_MEMBERS) |>
  drop_na()

# Calculate correlation matrix
cor_matrix <- cor(numeric_for_corr)

# Convert to long format for ggplot
cor_long <- cor_matrix |>
  as.data.frame() |>
  rownames_to_column("var1") |>
  pivot_longer(-var1, names_to = "var2", values_to = "correlation")

# Plot heatmap
ggplot(cor_long, aes(x = var1, y = var2, fill = correlation)) +
  geom_tile() +
  geom_text(aes(label = round(correlation, 2)), size = 3) +
  scale_fill_gradient2(low = "steelblue", mid = "white", high = "tomato",
                       midpoint = 0, limits = c(-1, 1)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Matrix of Key Numeric Variables", x = NULL, y = NULL)
```

**Key Observations:**

- Strong positive correlations exist between loan-related amounts: AMT_CREDIT, AMT_ANNUITY, and AMT_GOODS_PRICE are highly correlated (0.77-0.99), indicating that larger loans come with proportionally larger annuity payments and goods prices
- Family variables CNT_CHILDREN and CNT_FAM_MEMBERS show strong correlation (0.88), as expected since children directly contribute to family size
- Time variables (DAYS_BIRTH, DAYS_EMPLOYED, DAYS_REGISTRATION) show moderate positive correlations with each other, suggesting older clients tend to have longer employment histories and registration periods
- Target correlations are weak across all variables shown, with the strongest being DAYS_BIRTH (-0.08), indicating that age alone is a weak predictor of default. This suggests that external source scores (analyzed below) or engineered features may be more predictive

### External Source Scores vs Target

The EXT_SOURCE variables are normalized scores from external data sources (e.g., credit bureaus). These scores are often among the most predictive features in credit risk models as they aggregate information from the applicant's broader credit history.

```{r}
df |>
  select(TARGET, EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3) |>
  mutate(TARGET = factor(TARGET, labels = c("Repaid", "Default"))) |>
  pivot_longer(-TARGET, names_to = "source", values_to = "score") |>
  drop_na() |>
  ggplot(aes(x = score, fill = TARGET)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~source) +
  scale_fill_manual(values = c("Repaid" = "steelblue", "Default" = "tomato")) +
  labs(title = "External Source Scores by Loan Outcome",
       x = "Score", y = "Density")
```

**Key Observations:**

- All three external sources show clear separation between defaulters and non-defaulters, with higher scores associated with loan repayment
- EXT_SOURCE_2 shows the strongest discriminative power with minimal distribution overlap
- EXT_SOURCE_1 and EXT_SOURCE_3 also separate the classes but with more overlap, suggesting moderate predictive value
- These variables are likely to be among the most important features in any predictive model

# Conclusion

This EDA reveals a moderately imbalanced dataset (~8% default rate) with strong predictive signals in the external source scores and demographic variables. Key data quality issues include placeholder values in DAYS_EMPLOYED and significant missing data in housing-related columns. The dataset is ready for modeling after addressing these issues, with external source scores, age, income type, and education level emerging as promising predictors of default risk.

## Assumptions and Limitations

- Single table analysis: This EDA focused on application_train.csv only; the supplementary tables (bureau, credit card, installments) contain additional predictive information
- Missing data mechanism: Missing data must be "randomly missing" otherwise imputation would introduce bias.
- Class imbalance: The 8% default rate means model evaluation should prioritize AUC-ROC over accuracy.

## Key Insights and Findings

1. Class Imbalance: Only ~8% of loans default, requiring careful handling during modeling (e.g., class weights, oversampling)
2. External Source Scores: The EXT_SOURCE variables show strong separation between defaulters and non-defaulters - these are likely the most predictive features
3. Income Type Risk: Unemployed and maternity leave applicants show higher default rates compared to working professionals and pensioners
4. Education Effect: Lower education levels correlate with higher default rates
5. Age Relationship: Older applicants tend to have lower default rates (negative correlation between DAYS_BIRTH and TARGET)
6. Employment Anomaly: ~55,000 records have DAYS_EMPLOYED = 365243 (a placeholder for unemployed/pensioners) - requires special handling
7. Missing Data: Many housing-related columns have >40% missing values, suggesting these features may have limited utility
8. Feature Engineering Opportunities: Age (from DAYS_BIRTH), employment duration (from DAYS_EMPLOYED), and credit-to-income ratio could be valuable derived features

## Recommended Actions

Based on the EDA findings, the following cleaning steps are recommended for the modeling phase:

| Issue | Columns Affected | Recommended Action |
|-------|-----------------|-------------------|
| Missing values (>50%) | COMMONAREA_*, LANDAREA_*, etc. | Consider dropping or imputing with indicator flag |
| Missing values (<50%) | AMT_ANNUITY, AMT_GOODS_PRICE, etc. | Median imputation for numeric, mode for categorical |
| Placeholder values | DAYS_EMPLOYED = 365243 | Replace with NA or create binary flag for "unemployed" |
| Extreme outliers | AMT_INCOME_TOTAL (117M) | Cap at 99th percentile or log transform |
| XNA gender | CODE_GENDER | Recode as "Unknown" or impute based on other features |
| Categorical encoding | NAME_* columns | One-hot encoding or target encoding. Use one-hot for low cardinality. Target for high cardinaliity. |

## AI Use Disclosure

- I drafted an outline with headings for each section of the EDA notebook. I asked ChatGPT to review the outlines and headings. It pointed out some redundant headings and recommended a reordering for some.
- I was not able to get Databot to work due to a breaking change in the latest version of Positron requiring a Anthropic API key with credits. For Anthropic, I have access to Claude Code through work so I used that instead. Positron assistant was not the best alternative as it limited me to only the chat optimized models. I set claude code's output style to learning so that it would request me to write certain code sections that help cement conceptual understanding.
- I used claude code to generate the summary table in the data subscription section.
- I used claude code to sort the columns by logical category and print a visualization.
- I used claude code to generate the complex graphs and to improve visual clarity.
- I used claude code to generate the correlation matrix table and the grouped pivot table to visualize trends in the dataset.